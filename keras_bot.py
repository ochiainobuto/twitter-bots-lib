# -*- coding:utf-8 -*-
import tweepy
import emoji
from janome.tokenizer import Tokenizer
import collections
import random

word_list = [
    'å‰åŸ', 'ãƒ›ã‚¹ãƒˆ', 'ãƒ›ã‚¹ã‚¯ãƒ©', 'ãƒ›ã‚¹ç‹‚ã„', 'ã‚¯ã‚½ãƒªãƒ—', 'ãƒ¡ãƒ³ãƒ˜ãƒ©', 'ã‚½ãƒ¼ãƒ—å¬¢', 'ã‚ªãƒ•ãƒ‘ã‚³', 'ãƒ”ãƒ³ã‚µãƒ­', 'é¢¨ä¿—',
    'æ­Œèˆä¼ç”º', 'é‡‘æ´¥åœ’', 'ãƒªã‚¹ã‚«', 'è‡ªå‚·', 'SM', 'Mç”·', 'Sç”·', 'Må¥³', 'Så¥³', 'å¥³ç‹æ§˜', 'åˆºé’',
    'è‡ªå‚·', 'SM', 'ä¾å­˜ç—‡', 'ç‹¬å æ¬²', 'å«‰å¦¬', 'ã‹ã¾ã£ã¦ã¡ã‚ƒã‚“', 'æ­»ã«ãŸã„', 'é¬±', 'Mæ€§æ„Ÿ', 'æ‰‹ã‚³ã‚­',
    'ã‚ªãƒŠã‚¯ãƒ©', 'ãŠã£ãƒ‘ãƒ–', 'ã‚»ã‚¯ã‚­ãƒ£ãƒ', 'ãƒ©ãƒ–ãƒ›', 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ãƒ˜ãƒ«ã‚¹', 'æ•´å½¢', 'ã‚¯ãƒ³ãƒ‹', 'å¤ªå®¢', 'Mæ€§æ„Ÿ',
    'å·¨ä¹³', 'ç—´å¥³', 'é›»ãƒ', 'ãƒ•ã‚§ãƒ†ã‚£ãƒƒã‚·ãƒ¥ãƒãƒ¼', 'ç·Šç¸›', 'æ³¡å§«', 'ãƒ¡ãƒ³ã‚¨ã‚¹', 'ã‚»ãƒ•ãƒ¬', 'æœ¬æŒ‡', 'ãƒãƒ—ãƒãƒ¼',
    'é™°ã‚­ãƒ£', 'ã‚¢ãƒŠãƒ«', 'ãƒ‡ãƒªãƒ˜ãƒ«'
]

serch_word = random.choice(word_list)


def remove_emoji(src_str):
    return ''.join(c for c in src_str if c not in emoji.UNICODE_EMOJI)


tweet_list = []
emojis = ' ğŸ¤—â­•ğŸ¤“ğŸ¤”ğŸ¤˜ğŸ¦â­ğŸ†—ğŸ†–ğŸˆ²ğŸ¤ğŸ¤—ğŸ¤–ğŸ¤‘ğŸ†™â©'

CK = 'owmfVDnAGuHYaTLiO59cyxLhY'
CS = 'RvZzePeciSmSFQbkQyxuNy8N0YWURKAG2ldgd0lW0FhVWssE52'
AT = '1254252741502709765-YSQKdjttlT37suWU3AgkdEGS2NNyEb'
AS = 'FqIMD94c7BjCcc08WNlAgJ3gXIEePUedM3G4tQYkzTARx'

auth = tweepy.OAuthHandler(CK, CS)
auth.set_access_token(AT, AS)
api = tweepy.API(auth)

tweet_data = api.search(q=serch_word, count=100)

for tweet in tweet_data:
    #     print(tweet.text)
    tweet_text = tweet.text.replace('\n', '')
    tweet_text = tweet_text.replace('\u3000', '')
    tweet_text = tweet_text.replace('\u200dï¸', '')
    tweet_text = tweet_text.replace('ï¼¼', '')
    tweet_text = tweet_text.replace('https', '')

    tweet_text = remove_emoji(tweet_text)

    tweet_split = tweet_text.split(':')

    if len(tweet_split) > 1:
        tweet_list.append(tweet_split[1])
    else:
        tweet_list.append(tweet_text)

    remove_list = []
    for line in tweet_list:
        #         print(line)
        if len(line) > 0:
            if line[0] == 'ï¼ƒ':
                remove_list.append(line)
            if line[0] == '@':
                remove_list.append(line)
            if line[0] == '/' and line[1] == '/':
                remove_list.append(line)
            if line[0] == 'R' and line[1] == 'T':
                remove_list.append(line)

    for line in remove_list:
        tweet_list.remove(line)

next_max_id = tweet_data[-1].id
for i in range(2, 110):
    try:
        tweet_data = api.search(q=serch_word,
                                count=100,
                                max_id=next_max_id - 1)
        next_max_id = tweet_data[-1].id
        for tweet in tweet_data:
            tweet_text = tweet.text.replace('\n', '')
            tweet_text = tweet_text.replace('\u3000', '')
            tweet_text = tweet_text.replace('\u200dï¸', '')
            tweet_text = tweet_text.replace('ï¼¼', '')
            tweet_text = tweet_text.replace('https', '')

            tweet_text = remove_emoji(tweet_text)

            tweet_split = tweet_text.split(':')

            if len(tweet_split) > 1:
                tweet_list.append(tweet_split[1])
            else:
                tweet_list.append(tweet_text)

            remove_list = []
            for line in tweet_list:
                #         print(line)
                if len(line) > 0:
                    if line[0] == 'ï¼ƒ':
                        remove_list.append(line)
                    if line[0] == '@':
                        remove_list.append(line)
                    if line[0] == '/' and line[1] == '/':
                        remove_list.append(line)
                    if line[0] == 'R' and line[1] == 'T':
                        remove_list.append(line)

            for line in remove_list:
                tweet_list.remove(line)
    except:
        pass

##### åè©ã‚’æŠœãå‡ºã™ #####
tokenizer = Tokenizer()
noun_list = []

for sentence in tweet_list:
    for token in tokenizer.tokenize(sentence):
        word = str(token).split('\t')
        if word[1].split(',')[0] == "åè©" and word[1].split(',')[1] == "ä¸€èˆ¬":
            noun_list.append(word[0])

c = collections.Counter(noun_list)
_c = c.most_common()

tweet_word = 'Twitterã§ã€Œ' + serch_word + 'ã€ã‚’æ¤œç´¢ã—ãŸã¨ãã«å¤šãå«ã¾ã‚Œã‚‹åè©ãƒ™ã‚¹ãƒˆï¼‘ï¼---'
for i in range(1, 11):
    tweet_word = tweet_word + 'â– ' + str(i) + 'ä½: ' + str(_c[i][0]) + ':' + str(
        _c[i][1]) + 'å› '

api.update_status(tweet_word)